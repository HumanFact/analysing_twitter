{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#Import pandas and numpy to handle data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import libraries for accessing the database\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from postgres_credentials import *\n",
    "\n",
    "#import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import libraries for tokenization and ML\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer;\n",
    "\n",
    "\n",
    "#Import all libraries for creating a deep neural network\n",
    "#Sequential is the standard type of neural network with stackable layers\n",
    "from keras.models import Sequential;\n",
    "#Dense: Standard layers with every node connected, dropout: avoids overfitting\n",
    "from keras.layers import Dense, Dropout, Activation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying the database\n",
    "def query_database(tabletweets):\n",
    "    engine = create_engine(\"postgresql+psycopg2://%s:%s@%s:%d/%s\" %(usertwitter, passwordtwitter, hosttwitter, porttwitter, dbnametwitter))\n",
    "    table = pd.read_sql_query('select * from %s' %tabletweets,con=engine, index_col='id')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess text in tweets by removing links, @UserNames, blank spaces, etc.\n",
    "def preprocessing_text():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the data: Tokenization\n",
    "def tokenization_tweets(table):\n",
    "    tokenization = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\n",
    "    tokenization.fit_on_texts(table['tweet'])\n",
    "    return tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset\n",
    "dictionary = tokenization_tweets(query_database('tweets_avengers'))\n",
    "a = dictionary.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2: Create a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "model_nn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
