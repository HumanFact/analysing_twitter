{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "#Module to handle regular expressions\n",
    "import re\n",
    "#Library for emoji\n",
    "import emoji\n",
    "#Import pandas and numpy to handle data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import libraries for accessing the database\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from postgres_credentials import *\n",
    "\n",
    "#import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Import nltk to check english lexicon\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import (\n",
    "    wordnet,\n",
    "    stopwords\n",
    ")\n",
    "\n",
    "#import libraries for tokenization and ML\n",
    "import json;\n",
    "import keras;\n",
    "import keras.preprocessing.text as kpt;\n",
    "#from keras.preprocessing.text import Tokenizer;\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import all libraries for creating a deep neural network\n",
    "#Sequential is the standard type of neural network with stackable layers\n",
    "from keras.models import Sequential;\n",
    "#Dense: Standard layers with every node connected, dropout: avoids overfitting\n",
    "from keras.layers import Dense, Dropout, Activation;\n",
    "\n",
    "#To anotate database\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying the database\n",
    "def query_database(tabletweets):\n",
    "    engine = create_engine(\"postgresql+psycopg2://%s:%s@%s:%d/%s\" %(usertwitter, passwordtwitter, hosttwitter, porttwitter, dbnametwitter))\n",
    "    table = pd.read_sql_query('select * from %s' %tabletweets,con=engine, index_col='id')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anotate_dataframe(row):\n",
    "    nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    a = []\n",
    "    res = nlp.annotate(row,\n",
    "                           properties={\n",
    "                               'annotators': 'sentiment',\n",
    "                               'outputFormat': 'json',\n",
    "                               'timeout': 1000,\n",
    "                           })\n",
    "    \n",
    "    for s in res[\"sentences\"]:\n",
    "        a = s[\"sentiment\"]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess text in tweets by removing links, @UserNames, blank spaces, etc.\n",
    "def preprocessing_text(table):\n",
    "    #put everythin in lowercase\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    #Replace rt indicating that was a retweet\n",
    "    table['tweet'] = table['tweet'].str.replace('rt', '')\n",
    "    #Replace occurences of mentioning @UserNames\n",
    "    table['tweet'] = table['tweet'].replace(r'@\\w+', '', regex=True)\n",
    "    #Replace links contained in the tweet\n",
    "    table['tweet'] = table['tweet'].replace(r'http\\S+', '', regex=True)\n",
    "    table['tweet'] = table['tweet'].replace(r'www.[^ ]+', '', regex=True)\n",
    "    #remove numbers\n",
    "    table['tweet'] = table['tweet'].replace(r'[0-9]+', '', regex=True)\n",
    "    #replace special characters and puntuation marks\n",
    "    table['tweet'] = table['tweet'].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
    "    return table    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace elongated words by identifying those repeated characters and then remove them and compare the new word with the english lexicon\n",
    "def in_dict(word):\n",
    "    if wordnet.synsets(word):\n",
    "        #if the word is in the dictionary, we'll return True\n",
    "        return True\n",
    "\n",
    "def replace_elongated_word(word):\n",
    "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
    "    repl = r'\\1\\2\\3'    \n",
    "    if in_dict(word):\n",
    "        return word\n",
    "    new_word = re.sub(regex, repl, word)\n",
    "    if new_word != word:\n",
    "        return replace_elongated_word(new_word)\n",
    "    else:\n",
    "        return new_word\n",
    "\n",
    "def detect_elongated_words(row):\n",
    "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
    "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
    "    for word in words:\n",
    "        if not in_dict(word):\n",
    "            row = re.sub(word, replace_elongated_word(word), row)\n",
    "    return row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(table):\n",
    "    #We need to remove the stop words\n",
    "    stop_words_list = stopwords.words('english')\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_antonyms(word):\n",
    "    #We get all the lemma for the word\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for lemma in syn.lemmas(): \n",
    "            #if the lemma is an antonyms of the word\n",
    "            if lemma.antonyms(): \n",
    "                #we return the antonym\n",
    "                return lemma.antonyms()[0].name()\n",
    "    return word\n",
    "            \n",
    "def handling_negation(row):\n",
    "    #Tokenize the row\n",
    "    words = word_tokenize(row)\n",
    "    speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
    "    #We obtain the type of words that we have in the text, we use the pos_tag function\n",
    "    tags = nltk.pos_tag(words)\n",
    "    #Now we ask if we found a negation in the words\n",
    "    tags_2 = ''\n",
    "    if \"n't\" in words and \"not\" in words:\n",
    "        tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "        words = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "    elif \"n't\" in words:\n",
    "        tags_2 = tags[words.index(\"n't\"):]\n",
    "        words = words[words.index(\"n't\"):] \n",
    "    elif \"not\" in words:\n",
    "        tags_2 = tags[words.index(\"not\"):]\n",
    "        words = words[words.index(\"not\"):]\n",
    "        \n",
    "    for index, word_tag in enumerate(tags_2):\n",
    "        if word_tag[1] in speach_tags:\n",
    "            words = words[:index]+[replace_antonyms(word_tag[0])]+words[index+1:]\n",
    "            break\n",
    "            \n",
    "    return ' '.join(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_table(table):\n",
    "    #This function will process all the required cleaning for the text in our tweets\n",
    "    table = preprocessing_text(table)\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: detect_elongated_words(x))\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: handling_negation(x))\n",
    "    table = stop_words(table)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization for Data Visualization\n",
    "def vectorization(table):\n",
    "    #CountVectorizer will convert a collection of text documents to a matrix of token counts\n",
    "    #Produces a sparse representation of the counts \n",
    "    #Initialize\n",
    "    vector = CountVectorizer()\n",
    "    #We fit and transform the vector created\n",
    "    frequency_matrix = vector.fit_transform(table.tweet)\n",
    "    #Sum all the frequencies for each word\n",
    "    sum_frequencies = np.sum(frequency_matrix, axis=0)\n",
    "    #Now we use squeeze to remove single-dimensional entries from the shape of an array that we got from applying np.asarray to\n",
    "    #the sum of frequencies.\n",
    "    frequency = np.squeeze(np.asarray(sum_frequencies))\n",
    "    #Now we get into a dataframe all the frequencies and the words that they correspond to\n",
    "    frequency_df = pd.DataFrame([frequency], columns=vector.get_feature_names()).transpose()\n",
    "    return frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(tweet_table):\n",
    "    tweets_list = pd.Series([t for t in tweet_table.tweet]).str.cat(sep=' ')\n",
    "    wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(tweets_list)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(table, sent):\n",
    "    word_frequency = vectorization(table).sort_values(0, ascending = False)\n",
    "    labels = word_frequency[0][1:51].index\n",
    "    title = 'Word Frequency for %s' %sent\n",
    "    #Plot the figures\n",
    "    plt.figure()\n",
    "    barfreq = plt.bar(np.arange(50), word_frequency[0][1:51], width = 0.8, color = sns.color_palette(\"bwr\"), alpha=0.5, edgecolor = 'black', capsize=8, linewidth=1);\n",
    "    plt.xticks(np.arange(50), labels, rotation=90, size=14);\n",
    "    plt.xlabel('50 more frequent words', size=14);\n",
    "    plt.ylabel('Frequency', size=14);\n",
    "    #plt.title('Word Frequency for %s', size=18) %sent;\n",
    "    plt.title(title, size=18)\n",
    "    plt.grid(False);\n",
    "    plt.gca().spines['top'].set_visible(False);\n",
    "    plt.gca().spines['right'].set_visible(False);\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into training and test dataset\n",
    "def splitting(table):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(table.tweet, table.sentiment, test_size=0.2, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization for analysis\n",
    "def tokenization_tweets(dataset):\n",
    "    tokenization = TfidfVectorizer(max_features=50)\n",
    "    tokenization.fit(dataset)\n",
    "    dataset_transformed = tokenization.transform(dataset).toarray()\n",
    "    return dataset_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Neural Network\n",
    "#Create the model\n",
    "def train(X_train_mod, y_train):\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(512, input_shape=(50,), activation='relu'))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(256, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "\n",
    "    model_nn.compile(loss='binary_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    model_nn.fit(np.array(X_train_mod), y_train,\n",
    "                 batch_size=32,\n",
    "                 epochs=5,\n",
    "                 verbose=1,\n",
    "                 validation_split=0.1,\n",
    "                 shuffle=True)\n",
    "    return model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, model_nn):\n",
    "    prediction = model_nn.predict(X_test)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tabletweets = 'tweets_avengers'\n",
    "    tweet_table = query_database(tabletweets)\n",
    "    tweet_table = cleaning_table(tweet_table) \n",
    "    table['sentiment'] = table['tweet'].apply(lambda x: anotate_dataframe(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    #First we draw a word cloud\n",
    "    #For All tweets\n",
    "    word_cloud(pd.Series([t for t in tweet_table.tweet]).str.cat(sep=' '))    \n",
    "    #For positive tweets \n",
    "    word_cloud(pd.Series([t for t in tweet_table[tweet_table.sentiment == 'positive'].tweet]).str.cat(sep=' '))   \n",
    "    #For negative tweets\n",
    "    word_cloud(pd.Series([t for t in tweet_table[tweet_table.sentiment == 'negative'].tweet]).str.cat(sep=' '))\n",
    "    \n",
    "    \n",
    "    #Graph with frequency words all, positive and negative tweets and get the frequency\n",
    "    graph(tweet_table, 'all')\n",
    "    graph(tweet_table[tweet_table['sentiment'] == 'Positive'], 'positive')\n",
    "    graph(tweet_table[tweet_table['sentiment'] == 'Pegative'], 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, X_test, y_train, y_test = splitting(tweet_table)\n",
    "    X_train_mod = tokenization_tweets(X_train)\n",
    "    model = train(X_train_mod, y_train)\n",
    "    test(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
