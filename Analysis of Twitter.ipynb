{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "#Module to handle regular expressions\n",
    "import re\n",
    "#Library for emoji\n",
    "import emoji\n",
    "#Import pandas and numpy to handle data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import libraries for accessing the database\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from postgres_credentials import *\n",
    "\n",
    "#import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Import nltk to check english lexicon\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#import libraries for tokenization and ML\n",
    "import json;\n",
    "import keras;\n",
    "import keras.preprocessing.text as kpt;\n",
    "#from keras.preprocessing.text import Tokenizer;\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import all libraries for creating a deep neural network\n",
    "#Sequential is the standard type of neural network with stackable layers\n",
    "from keras.models import Sequential;\n",
    "#Dense: Standard layers with every node connected, dropout: avoids overfitting\n",
    "from keras.layers import Dense, Dropout, Activation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying the database\n",
    "def query_database(tabletweets):\n",
    "    engine = create_engine(\"postgresql+psycopg2://%s:%s@%s:%d/%s\" %(usertwitter, passwordtwitter, hosttwitter, porttwitter, dbnametwitter))\n",
    "    table = pd.read_sql_query('select * from %s' %tabletweets,con=engine, index_col='id')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess text in tweets by removing links, @UserNames, blank spaces, etc.\n",
    "def preprocessing_text(table):\n",
    "    #put everythin in lowercase\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    #Replace rt indicating that was a retweet\n",
    "    table['tweet'] = table['tweet'].str.replace('rt', '')\n",
    "    #Replace occurences of mentioning @UserNames\n",
    "    table['tweet'] = table['tweet'].replace(r'@\\w+', '', regex=True)\n",
    "    #Replace links contained in the tweet\n",
    "    table['tweet'] = table['tweet'].replace(r'http\\S+', '', regex=True)\n",
    "    table['tweet'] = table['tweet'].replace(r'www.[^ ]+', '', regex=True)\n",
    "    #remove numbers\n",
    "    table['tweet'] = table['tweet'].replace(r'[0-9]+', '', regex=True)\n",
    "    #replace special characters and puntuation marks\n",
    "    table['tweet'] = table['tweet'].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
    "    return table    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace elongated words by identifying those repeated characters and then remove them and compare the new word with the english lexicon\n",
    "def in_dict(word):\n",
    "    if wordnet.synsets(word):\n",
    "        #if the word is in the dictionary, we'll return True\n",
    "        return True\n",
    "\n",
    "def replace_elongated_word(word):\n",
    "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
    "    repl = r'\\1\\2\\3'    \n",
    "    if in_dict(word):\n",
    "        return word\n",
    "    new_word = re.sub(regex, repl, word)\n",
    "    if new_word != word:\n",
    "        return replace_elongated_word(new_word)\n",
    "    else:\n",
    "        return new_word\n",
    "\n",
    "def detect_elongated_words(row):\n",
    "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
    "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
    "    for word in words:\n",
    "        if not in_dict(word):\n",
    "            row = re.sub(word, replace_elongated_word(word), row)\n",
    "    return row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(table):\n",
    "    #We need to remove the stop words\n",
    "    stop_words_list = stopwords.words('english')\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def handling_negation(row):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def replace_contractions():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def stemming_tweets():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove duplicates():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_table(table):\n",
    "    #This function will process all the required cleaning for the text in our tweets\n",
    "    table = preprocessing_text(table)\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: detect_elongated_words(x))\n",
    "    table = stop_words(table)\n",
    "    #table = handling_negation()\n",
    "    #table = replace_contractions()\n",
    "    #table = stemming_tweets()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization for Data Visualization\n",
    "def vectorization(table):\n",
    "    #CountVectorizer will convert a collection of text documents to a matrix of token counts\n",
    "    #Produces a sparse representation of the counts \n",
    "    #Initialize\n",
    "    vector = CountVectorizer()\n",
    "    #We fit and transform the vector created\n",
    "    frequency_matrix = vector.fit_transform(table.tweet)\n",
    "    #Sum all the frequencies for each word\n",
    "    sum_frequencies = np.sum(frequency_matrix, axis=0)\n",
    "    #Now we use squeeze to remove single-dimensional entries from the shape of an array that we got from applying np.asarray to\n",
    "    #the sum of frequencies.\n",
    "    frequency = np.squeeze(np.asarray(sum_frequencies))\n",
    "    #Now we get into a dataframe all the frequencies and the words that they correspond to\n",
    "    frequency_df = pd.DataFrame([frequency], columns=vector.get_feature_names()).transpose()\n",
    "    return frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into training and test dataset\n",
    "def splitting(table):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(table.tweet, table.sentiment, test_size=0.33, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization for analysis\n",
    "def tokenization_tweets(dataset):\n",
    "    tokenization = TfidfVectorizer(max_features=50)\n",
    "    tokenization.fit(dataset)\n",
    "    dataset_transformed = tokenization.transform(dataset).toarray()\n",
    "    return dataset_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Neural Network\n",
    "#Create the model\n",
    "def train(X_train_mod, y_train):\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(512, input_shape=(50,), activation='relu'))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(256, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "\n",
    "    model_nn.compile(loss='binary_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    model_nn.fit(np.array(X_train_mod), y_train,\n",
    "                 batch_size=32,\n",
    "                 epochs=5,\n",
    "                 verbose=1,\n",
    "                 validation_split=0.1,\n",
    "                 shuffle=True)\n",
    "    return model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, model_nn):\n",
    "    prediction = model_nn.predict(X_test)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tabletweets = 'tweets_avengers'\n",
    "    tweet_table = query_database(tabletweets)\n",
    "    tweet_table = cleaning_table(tweet_table) \n",
    "    \n",
    "    #First we draw a word cloud\n",
    "    #For All tweets\n",
    "    tweets_list = pd.Series([t for t in tweet_table.tweet]).str.cat(sep=' ')\n",
    "    wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(tweets_list)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() \n",
    "    \n",
    "    #For positive tweets\n",
    "    \n",
    "    #For negative tweets\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Graph with frequency words\n",
    "    #Vectorize all, positive and negative tweets and get the frequency\n",
    "    word_frequency = vectorization(tweet_table).sort_values(0, ascending = False)\n",
    "    wordfrequency_positive = vectorization(tweet_table[tweet_table['sentiment'] == 1]).sort_values(0, ascending = False)\n",
    "    wordfrequency_negative = vectorization(tweet_table[tweet_table['sentiment'] == -1]).sort_values(0, ascending = False)\n",
    "    \n",
    "    #Get labels (words) for all, pos and neg tweets\n",
    "    labels = word_frequency[0][1:51].index\n",
    "    labels_positive = word_frequency_positive[0][1:51].index\n",
    "    labels_negative = word_frequency_positive[0][1:51].index\n",
    "    \n",
    "    #Plot the figures\n",
    "    plt.subplots(2, 2, sharex=False, sharey=False)\n",
    "    barfreq = plt.bar(np.arange(50), word_frequency[0][1:51], width = 0.8, color = sns.color_palette(\"bwr\"), alpha=0.5, edgecolor = 'black', capsize=8, linewidth=1);\n",
    "    plt.xticks(np.arange(50), labels, rotation=90, size=14);\n",
    "    plt.xlabel('50 more frequent words', size=14);\n",
    "    plt.ylabel('Frequency', size=14);\n",
    "    plt.title('Word Frequency', size=18);\n",
    "    plt.grid(False);\n",
    "    plt.gca().spines['top'].set_visible(False);\n",
    "    plt.gca().spines['right'].set_visible(False);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, X_test, y_train, y_test = splitting(tweet_table)\n",
    "    X_train_mod = tokenization_tweets(X_train)\n",
    "    model = train(X_train_mod, y_train)\n",
    "    test(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
