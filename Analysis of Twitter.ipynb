{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "#Module to handle regular expressions\n",
    "import re\n",
    "#Library for emoji\n",
    "import emoji\n",
    "#Import pandas and numpy to handle data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import libraries for accessing the database\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from postgres_credentials import *\n",
    "\n",
    "#import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Import nltk to check english lexicon\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import (\n",
    "    wordnet,\n",
    "    stopwords\n",
    ")\n",
    "\n",
    "#import libraries for tokenization and ML\n",
    "import json;\n",
    "import keras;\n",
    "import keras.preprocessing.text as kpt;\n",
    "#from keras.preprocessing.text import Tokenizer;\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import all libraries for creating a deep neural network\n",
    "#Sequential is the standard type of neural network with stackable layers\n",
    "from keras.models import Sequential;\n",
    "#Dense: Standard layers with every node connected, dropout: avoids overfitting\n",
    "from keras.layers import Dense, Dropout, Activation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying the database\n",
    "def query_database(tabletweets):\n",
    "    engine = create_engine(\"postgresql+psycopg2://%s:%s@%s:%d/%s\" %(usertwitter, passwordtwitter, hosttwitter, porttwitter, dbnametwitter))\n",
    "    table = pd.read_sql_query('select * from %s' %tabletweets,con=engine, index_col='id')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess text in tweets by removing links, @UserNames, blank spaces, etc.\n",
    "def preprocessing_text(table):\n",
    "    #put everythin in lowercase\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    #Replace rt indicating that was a retweet\n",
    "    table['tweet'] = table['tweet'].str.replace('rt', '')\n",
    "    #Replace occurences of mentioning @UserNames\n",
    "    table['tweet'] = table['tweet'].replace(r'@\\w+', '', regex=True)\n",
    "    #Replace links contained in the tweet\n",
    "    table['tweet'] = table['tweet'].replace(r'http\\S+', '', regex=True)\n",
    "    table['tweet'] = table['tweet'].replace(r'www.[^ ]+', '', regex=True)\n",
    "    #remove numbers\n",
    "    table['tweet'] = table['tweet'].replace(r'[0-9]+', '', regex=True)\n",
    "    #replace special characters and puntuation marks\n",
    "    table['tweet'] = table['tweet'].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
    "    return table    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace elongated words by identifying those repeated characters and then remove them and compare the new word with the english lexicon\n",
    "def in_dict(word):\n",
    "    if wordnet.synsets(word):\n",
    "        #if the word is in the dictionary, we'll return True\n",
    "        return True\n",
    "\n",
    "def replace_elongated_word(word):\n",
    "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
    "    repl = r'\\1\\2\\3'    \n",
    "    if in_dict(word):\n",
    "        return word\n",
    "    new_word = re.sub(regex, repl, word)\n",
    "    if new_word != word:\n",
    "        return replace_elongated_word(new_word)\n",
    "    else:\n",
    "        return new_word\n",
    "\n",
    "def detect_elongated_words(row):\n",
    "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
    "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
    "    for word in words:\n",
    "        if not in_dict(word):\n",
    "            row = re.sub(word, replace_elongated_word(word), row)\n",
    "    return row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(table):\n",
    "    #We need to remove the stop words\n",
    "    stop_words_list = stopwords.words('english')\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_antonyms(word):\n",
    "    #We get all the lemma for the word\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for lemma in syn.lemmas(): \n",
    "            #if the lemma is an antonyms of the word\n",
    "            if lemma.antonyms(): \n",
    "                #we return the antonym\n",
    "                return lemma.antonyms()[0].name()\n",
    "            \n",
    "def handling_negation(row):\n",
    "    #Tokenize the row\n",
    "    words = word_tokenize(row)\n",
    "    #We obtain the type of words that we have in the text, we use the pos_tag function\n",
    "    tags = nltk.pos_tag(words)\n",
    "    #Now we ask if we found a negation in the words\n",
    "    tags_2 = ''\n",
    "    if \"n't\" in words and \"not\" in words:\n",
    "        tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "        words = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "    elif \"n't\" in words:\n",
    "        tags_2 = tags[words.index(\"n't\"):]\n",
    "        words = words[words.index(\"n't\"):] \n",
    "    elif \"not\" in words:\n",
    "        tags_2 = tags[words.index(\"not\"):]\n",
    "        words = words[words.index(\"not\"):]\n",
    "        \n",
    "    for index, word_tag in enumerate(tags):\n",
    "        if word_tag[1] == 'JJ' or word_tag[1] == 'JJR' or word_tag[1] == 'JJS':\n",
    "            words = words[:index]+[replace_antonyms(word_tag[0])]+words[index+1:]\n",
    "            break\n",
    "            \n",
    "    return ' '.join(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove duplicates():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_table(table):\n",
    "    #This function will process all the required cleaning for the text in our tweets\n",
    "    table = preprocessing_text(table)\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: detect_elongated_words(x))\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: handling_negation(x))\n",
    "    table = stop_words(table)\n",
    "    #table = stemming_tweets()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization for Data Visualization\n",
    "def vectorization(table):\n",
    "    #CountVectorizer will convert a collection of text documents to a matrix of token counts\n",
    "    #Produces a sparse representation of the counts \n",
    "    #Initialize\n",
    "    vector = CountVectorizer()\n",
    "    #We fit and transform the vector created\n",
    "    frequency_matrix = vector.fit_transform(table.tweet)\n",
    "    #Sum all the frequencies for each word\n",
    "    sum_frequencies = np.sum(frequency_matrix, axis=0)\n",
    "    #Now we use squeeze to remove single-dimensional entries from the shape of an array that we got from applying np.asarray to\n",
    "    #the sum of frequencies.\n",
    "    frequency = np.squeeze(np.asarray(sum_frequencies))\n",
    "    #Now we get into a dataframe all the frequencies and the words that they correspond to\n",
    "    frequency_df = pd.DataFrame([frequency], columns=vector.get_feature_names()).transpose()\n",
    "    return frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into training and test dataset\n",
    "def splitting(table):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(table.tweet, table.sentiment, test_size=0.33, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization for analysis\n",
    "def tokenization_tweets(dataset):\n",
    "    tokenization = TfidfVectorizer(max_features=50)\n",
    "    tokenization.fit(dataset)\n",
    "    dataset_transformed = tokenization.transform(dataset).toarray()\n",
    "    return dataset_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Neural Network\n",
    "#Create the model\n",
    "def train(X_train_mod, y_train):\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(512, input_shape=(50,), activation='relu'))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(256, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "\n",
    "    model_nn.compile(loss='binary_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    model_nn.fit(np.array(X_train_mod), y_train,\n",
    "                 batch_size=32,\n",
    "                 epochs=5,\n",
    "                 verbose=1,\n",
    "                 validation_split=0.1,\n",
    "                 shuffle=True)\n",
    "    return model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, model_nn):\n",
    "    prediction = model_nn.predict(X_test)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tabletweets = 'tweets_avengers'\n",
    "    tweet_table = query_database(tabletweets)\n",
    "\n",
    "    tweet_table = cleaning_table(tweet_table) \n",
    "    \n",
    "    #First we draw a word cloud\n",
    "    #For All tweets\n",
    "    tweets_list = pd.Series([t for t in tweet_table.tweet]).str.cat(sep=' ')\n",
    "    wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(tweets_list)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() \n",
    "    \n",
    "    #For positive tweets\n",
    "    tweets_list = pd.Series([t for t in tweet_table.tweet]).str.cat(sep=' ')\n",
    "    wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(tweets_list)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() \n",
    "    \n",
    "    #For negative tweets\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Graph with frequency words\n",
    "    #Vectorize all, positive and negative tweets and get the frequency\n",
    "    word_frequency = vectorization(tweet_table).sort_values(0, ascending = False)\n",
    "    wordfrequency_positive = vectorization(tweet_table[tweet_table['sentiment'] == 'positive']).sort_values(0, ascending = False)\n",
    "    wordfrequency_negative = vectorization(tweet_table[tweet_table['sentiment'] == 'negative']).sort_values(0, ascending = False)\n",
    "    \n",
    "    #Get labels (words) for all, pos and neg tweets\n",
    "    labels = word_frequency[0][1:51].index\n",
    "    labels_positive = word_frequency_positive[0][1:51].index\n",
    "    labels_negative = word_frequency_positive[0][1:51].index\n",
    "    \n",
    "    #Plot the figures\n",
    "    plt.subplots(2, 2, sharex=False, sharey=False)\n",
    "    barfreq = plt.bar(np.arange(50), word_frequency[0][1:51], width = 0.8, color = sns.color_palette(\"bwr\"), alpha=0.5, edgecolor = 'black', capsize=8, linewidth=1);\n",
    "    plt.xticks(np.arange(50), labels, rotation=90, size=14);\n",
    "    plt.xlabel('50 more frequent words', size=14);\n",
    "    plt.ylabel('Frequency', size=14);\n",
    "    plt.title('Word Frequency', size=18);\n",
    "    plt.grid(False);\n",
    "    plt.gca().spines['top'].set_visible(False);\n",
    "    plt.gca().spines['right'].set_visible(False);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, X_test, y_train, y_test = splitting(tweet_table)\n",
    "    X_train_mod = tokenization_tweets(X_train)\n",
    "    model = train(X_train_mod, y_train)\n",
    "    test(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tweet_table\n",
    "table = table.tweet.drop_duplicates(keep='first').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1         video lee sugeun youtube channel subscribers s...\n",
       "2                   avengers trailer told spongebob friends\n",
       "3         ok watched avengers infinity war first time kn...\n",
       "4                                     always good know back\n",
       "5         making new oreo products used easy like “let’s...\n",
       "7         drawing board today scarletwitch avengers marv...\n",
       "11        theory avengers endgame trailer see scott van ...\n",
       "12        avengers wong genius care anyone else says😂😂😂 ...\n",
       "13                                      avengers age ultron\n",
       "14                                     seriously can't wait\n",
       "18                                    avengers mike mignola\n",
       "20        made avengers meme laugh away pain endgame tra...\n",
       "21        avengers endgame imax trailer hits theaters we...\n",
       "24                                             smh avengers\n",
       "25        didn’t chris evans along jeremy rener call bla...\n",
       "34        loss something watch best movies new netflix d...\n",
       "35        dory's avengers alison jack alone betrayed you...\n",
       "36        avengers second screen experience torila tomhi...\n",
       "38                                               📽 avengers\n",
       "42        anyone breathes josie's lizie's direction aven...\n",
       "43                  care marvel disney wait marvel disney 🤣\n",
       "45              long say teams he's reserve member avengers\n",
       "154550                                                     \n",
       "46        crap look who’s coming april still left avenge...\n",
       "47        avengers endgame inspired timeless promo trail...\n",
       "49                   loki em thor ragnarok loki em avengers\n",
       "53        heck thing john good ring opinion stop saying ...\n",
       "55        marvel “avengers infinity war ambitious crosso...\n",
       "59                liked video avengers la historia en video\n",
       "62        twitter hard like get people see hashtags seem...\n",
       "                                ...                        \n",
       "232064                         avengers infinity war robbed\n",
       "232065    avengers assembled winter soldier civil war in...\n",
       "232066    oscars visual effects — avengers infinity war ...\n",
       "232073             nice see getting recognized good show 👏🙌\n",
       "232089    'avengers endgame' new look pepper pots' rescu...\n",
       "232095              agree infinity war fye y’all gota chill\n",
       "232102    black panther going avengers infinity wars goo...\n",
       "232109    agree bp impact preach black culture okay movi...\n",
       "232111                          thanos needs hot yoga class\n",
       "232118    oscar nomination interesting movie i’l give os...\n",
       "232121    well that's expected otherwise going snap fing...\n",
       "232123    pérez announces retirement comics unbelievable...\n",
       "232129                               i’m gone disagree dawg\n",
       "232134                                    quality list bruh\n",
       "232137    unpopular opinion antman wasp should’ve nomina...\n",
       "232142    avengers infinity war way better movie black p...\n",
       "232145                      honestly become popular opinion\n",
       "232148    thunder road's uk release delayed week avenger...\n",
       "232150    every time anything featuring majority black c...\n",
       "232151              black panthers bit best pa infinity war\n",
       "232152    congratulations marvel studios' avengers infin...\n",
       "232160                           thanos met great king mitu\n",
       "232162      plot twist spider man spider verse better bofum\n",
       "232166    st academy awards 🏆 marvel nominations time co...\n",
       "232170    highest alcohol content avengers members tca c...\n",
       "232172    ignore fact wasp founding member ther avengers...\n",
       "232173    jeremy watch marvel truck coming behind jeremy...\n",
       "232175    damn right episode along came spider avengers ...\n",
       "232178    dil raju confirmed maharshi release date april...\n",
       "232179    opinion unpopular it’s stupid can’t wait till ...\n",
       "Name: tweet, Length: 57536, dtype: object"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
